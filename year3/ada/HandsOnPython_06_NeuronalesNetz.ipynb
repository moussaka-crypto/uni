{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbgYWScQSQ4-"
   },
   "source": [
    "Dieses Notebook zeigt, wie man ein spezielles Rekurrentes Neuronales Netz (LSTM) zur Vorhersage von Energieverbrauchsdaten trainieren kann. \n",
    "Hierfür werden die Bibliotheken keras, sowie sci-kit learn verwendet.\n",
    "Das Notebook orientiert sich an folgendem Tutorial: https://www.elab2go.de/demo-py5/ (Autoren: Prof. Dr. Eva Maria Kiss, B. Sc. Franc Willy Pouhela, M. Sc. Anke Welz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKCtHqefSPFj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# matplotlib und seaborn zum Plotten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn für Überwachtes Lernen\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import max_error\n",
    "\n",
    "# keras für Neuronale Netze\n",
    "import tensorflow as tf \n",
    "import keras as keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, Dropout \n",
    "from keras.utils.vis_utils import plot_model \n",
    "from keras import activations\n",
    "\n",
    "# Für das Darstellen von Bildern im SVG-Format\n",
    "import graphviz as gv\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmm976BpAi1i"
   },
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv('https://www.elab2go.de/demo-py5/opsd_2016-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXwyyg9jCTtF"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtUkldNHCU7B"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ruAL4TjMF-m"
   },
   "source": [
    "# Aufgabe 1: Datenvorbereitung\n",
    " \n",
    "\n",
    "* a) Überprüfen Sie, ob in dem Datensatz NaN enthalten sind. Falls ja, überlegen Sie sich, wie Sie damit am besten umgehen.\n",
    "*   b) Ist es sinnvoll, mit allen Merkmalen fortzufahren? Was müssen Sie tun um diese Frage beantworten zu können?\n",
    "*   c) Bringen Sie die Spalte \"Datum\" in ein geeignetes Format und indizieren Sie das dataframe mit dieser Spalte \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LprFtFGuCYQL"
   },
   "outputs": [],
   "source": [
    "# Frage 1 a) Überprüfen auf NaN\n",
    "\n",
    "# Um NaN-Werte im Datensatz zu überprüfen, können Sie die Methode data.isnull().sum() verwenden.\n",
    "# Wenn es NaN-Werte gibt, können Sie sie entweder durch geeignete Werte ersetzen (z. B. den Mittelwert der Spalte)\n",
    "# oder die Zeilen, die NaN-Werte enthalten, entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCrYJQHBNT2Y"
   },
   "outputs": [],
   "source": [
    "# Frage 1 b)\n",
    "\n",
    "#Um zu entscheiden, ob es sinnvoll ist, mit allen Merkmalen fortzufahren, \n",
    "#sollten Sie eine explorative Datenanalyse durchführen \n",
    "#und die Korrelationen zwischen den Merkmalen und dem Ziel (Energieverbrauch) untersuchen. \n",
    "#Sie könnten auch Modelle mit unterschiedlichen Merkmalssätzen trainieren und ihre Leistung vergleichen, \n",
    "#um zu entscheiden, welche Merkmale am besten zur Vorhersage des Energieverbrauchs beitragen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9IjK4isNOi6"
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYYRMjO6C6dY"
   },
   "outputs": [],
   "source": [
    "# Frage 1 c)\n",
    "\n",
    "# Um das Datumsformat zu ändern und den DataFrame damit zu indizieren, verwenden Sie:\n",
    "data['Datum'] = pd.to_datetime(data['Datum'])\n",
    "data.set_index('Datum', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhkcjBBsCsT1"
   },
   "outputs": [],
   "source": [
    "# Plotten des Stromverbrauchs. Der Stromverbrauch soll später mithilfe des Neuronalen Netzes vorhergesagt werden.\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12, 4)})\n",
    "sns.set_color_codes('bright')\n",
    "\n",
    "ax = data['Verbrauch'].plot(linewidth=1, color='b', marker = '.')\n",
    "ax.set_title('Täglicher Stromverbrauch')\n",
    "ax.set_xlabel('Datum');\n",
    "ax.set_ylabel('GwH');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-y0FKomOETF"
   },
   "source": [
    "# Aufgabe 2\n",
    "\n",
    "Im nächsten Codeblock werden zeitlich versetzte Merkmale (sogenannte Lagged Features) erzeugt.  \n",
    "\n",
    "*   a) Was macht dabei die Funktion shift()? Schauen Sie in der pandas Dokumentation nach!\n",
    "*   b) Was macht die Funktion concat()? Schauen Sie auch das in der pandas Dokumentation nach!\n",
    "*   c) Führen Sie die untenstehende Zelle aus und Lassen Sie sich anschließend die ersten 10 Zeilen des neu erzeugten dataframes ausgeben. Hat das Erzeugen der zeitlich versetzten Merkmale erfolgreich funktioniert?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXixVLD8UU6I"
   },
   "source": [
    "# Antworten\n",
    "\n",
    "\n",
    "\n",
    "* a) Die Funktion shift() verschiebt die Werte in einer pandas Series oder einem DataFrame entlang der angegebenen Achse um eine     bestimmte Anzahl von Perioden. In diesem Fall wird sie verwendet, um zeitlich versetzte Merkmale (Lagged Features) zu      erzeugen.\n",
    "*   b) Die Funktion concat() verbindet pandas-Objekte (Series oder DataFrames) entlang einer bestimmten Achse.\n",
    "*   c) Um die ersten 10 Zeilen des neu erzeugten DataFrames anzuzeigen, verwenden Sie data.head(10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38nuB_e8Fgyd"
   },
   "outputs": [],
   "source": [
    "# Erzeuge zeitlich versetzte Merkmale (lagged features)\n",
    "\n",
    "consumption = pd.DataFrame(data['Verbrauch'])\n",
    "\n",
    "data = pd.concat([consumption.shift(7),consumption.shift(6), consumption.shift(5), consumption.shift(4), \n",
    "                  consumption.shift(3), consumption.shift(2), consumption.shift(1), data[['Verbrauch']]], axis=1)\n",
    "\n",
    "data.columns =  ['t-7', 't-6', 't-5', 't-4', 't-3', 't-2', 't-1', 'Verbrauch']\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnvY2JYtTNXn"
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cITs1ZxLCzXQ"
   },
   "outputs": [],
   "source": [
    "# Daten in Trainings- und Testset aufspalten\n",
    "\n",
    "TEST_SPLIT = 0.1 \n",
    "#data = data.drop(columns = ['Wind', 'Solar'])\n",
    "train_size = int(len(data) * (1-test_split))\n",
    "test_size = len(data) - train_size\n",
    "train = data.iloc[0:train_size]\n",
    "test = data.iloc[train_size:len(data)]\n",
    "print(\"Trainingsdaten:\") \n",
    "print(train.head())\n",
    "print(train.tail())\n",
    "print(\"Testdaten:\") \n",
    "print(test.head())\n",
    "print(test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLkdRtBBDOOE"
   },
   "outputs": [],
   "source": [
    "# Daten auf den Wertebereich [0, 1] skalieren\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "train_s = scaler.fit_transform(np.array(train))\n",
    "test_s = scaler.fit_transform(np.array(test))\n",
    "\n",
    "print(\"Trainingsdaten (unskaliert)\\n\")\n",
    "print(train.head(3))\n",
    "print(\"\\nTrainingsdaten (skaliert)\\n\")\n",
    "train_s = pd.DataFrame( train_s , columns = data.columns)\n",
    "train_s = train_s.set_index(train.index )\n",
    "print(train_s.head(3))\n",
    "\n",
    "print(\"Testdaten (unskaliert)\\n\")\n",
    "print(test.head(3))\n",
    "print(\"\\nTestdaten (skaliert)\\n\")\n",
    "test_s = pd.DataFrame( test_s , columns = data.columns)\n",
    "test_s = test_s.set_index(test.index )\n",
    "print(test_s.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjYQ-fbzUvdu"
   },
   "outputs": [],
   "source": [
    "# In Merkmale und Label aufteilen\n",
    "\n",
    "X_train = train_s.drop(columns = ['Verbrauch'])\n",
    "y_train = pd.DataFrame(train_s['Verbrauch'])\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBpV9MC4EBSK"
   },
   "outputs": [],
   "source": [
    "TIMESTEPS = 7 # Länge eines gleitenden Zeitfensters\n",
    "UNITS = 10 # Ausgabedimension einer einzelnen LSTM-Schicht\n",
    "N_LAYER = 2 # Anzahl an LSTM-Schichten\n",
    "\n",
    "# Initialisiere ein sequentielles Modell (Modell mit mehreren Schichten)\n",
    "model = Sequential(name='sequential') \n",
    "\n",
    "#Füge so viele Schichten hinzu, wie in N_LAYER angegeben\n",
    "for i in range(N_LAYER):\n",
    "\n",
    "  lstm_layer = LSTM(units = UNITS, # Dimension der Ausgabe\n",
    "                    input_shape=(TIMESTEPS,1), # Dimension der Eingabe (1. Dimension entspricht Anzahl Zeitschritte, 2. Dimension:  1, da nur ein Merkmal = Verbrauch über die Zeit betrachtet wird (univariates Problem) )\n",
    "                    return_sequences=True, # wir wollen die gesamte Ausgabesequenz der jeweiligen LSTM-Schicht in die nächste LSTM-Schicht weitergeben können\n",
    "                    name = 'lstm_' + str(i+1)) # Schichten werden mit \"lstm_\" + Schichtnummer benannt\n",
    "                   \n",
    "  model.add(lstm_layer) # Schicht dem Modell hinzufügen\n",
    "\n",
    "\n",
    "# weitere LSTM Schicht hinzufügen, bei der nur die letzte Ausgabe (und nicht wie oben die ganze Sequenz) in die nächste Schicht weitergegeben wird\n",
    "model.add(LSTM(units = UNITS, input_shape=(TIMESTEPS,1), name = 'lstm_' + str(N_LAYER+1)))\n",
    "\n",
    "# Lineares Layer mit ReLU-Aktivierungsfunktion\n",
    "model.add(Dense(units = 1, name='dense_1'))#, activation=activations.tanh))\n",
    "\n",
    "# Konfiguriere das Modell für die Trainingsphase \n",
    "\n",
    "# Angeben welcher Optimierer verwendet werden soll\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# über welche Verlustfunktion optimiert werden soll \n",
    "# und wie die Performance des Modells gemessen werden soll (hier werden Verlustfunktion und Performancemetrik gleich gewählt, muss aber nicht so sein))\n",
    "model.compile(optimizer = opt, loss = \"mse\", metrics=['mean_squared_error'])\n",
    "\n",
    "# Zusammenfassung und Visualisierung des Modells\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrKru1jmRN3G"
   },
   "source": [
    "# Aufgabe 3\n",
    "\n",
    "Welchen Parameter der obigen Codezelle müssten Sie verändern, um 3 LSTM Schichten zu bekommen?\n",
    "\n",
    "->Um 3 LSTM-Schichten im Modell zu erhalten, ändern Sie den Wert von N_LAYER auf 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRrNMG51SAEt"
   },
   "source": [
    "# Aufgabe 4\n",
    "\n",
    "Das Neuronale Netz wird in der unten stehenden Codezelle trainiert. Führen Sie den Code aus. \n",
    "Nach wie vielen Epochen wird das Training auf jeden Fall stoppen?\n",
    "\n",
    "-> Das Training wird auf jeden Fall nach der Anzahl von Epochen stoppen, die in der Variable EPOCHS angegeben ist. In diesem Fall beträgt die Anzahl der Epochen 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk5LqeA9EFMd"
   },
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "\n",
    "# Erstelle Callback für Stop-Kriterium\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "cb_stop = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                        verbose=1, patience=200)\n",
    "log_file = 'demo-py5-log.csv'\n",
    "cb_logger = CSVLogger(log_file, append=False, separator=';')\n",
    "\n",
    "# X_train erhält eine zusätzliche Dimension und wird dreidimensional\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.reshape(X_train, \n",
    "                     (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# Trainiere das Modell mit Hilfe der Funktion fit()\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                    validation_split=TEST_SPLIT, verbose=2, \n",
    "                    callbacks=[cb_logger, cb_stop])\n",
    "\n",
    "# Speichere das Modell im Format HDF5\n",
    "model.save(\"model_adam.h5\")\n",
    "\n",
    "print(\"History\");print(history.history.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQA7oBZ6TP6X"
   },
   "source": [
    "# Aufgabe 5\n",
    "\n",
    "Im untenstehenden Codeblock wird der Trainingsfortschritt geplottet. Wie beurteilen Sie ihn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96-yfi5QmDBC"
   },
   "outputs": [],
   "source": [
    "# Trainingsfortschritt plotten\n",
    "\n",
    "plt.plot(history.history['mean_squared_error'], label='MSE (Trainingsdaten)')\n",
    "plt.plot(history.history['val_mean_squared_error'], label='MSE (Testdaten)')\n",
    "\n",
    "plt.title('Training: Entwicklung des Fehlers')\n",
    "plt.ylabel('MSE-Fehler')\n",
    "plt.xlabel('Epochen')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vl7Kw0kEmNsE"
   },
   "outputs": [],
   "source": [
    "# Testdaten in Merkmale X und Label y aufteilen\n",
    "\n",
    "X_test = test_s.drop(columns = ['Verbrauch'])\n",
    "y_test = pd.DataFrame(test_s['Verbrauch'])\n",
    "\n",
    "# Vorhersage für skalierte Testdaten\n",
    "X_test = np.array(X_test)\n",
    "X_test_input = np.reshape(X_test, \n",
    "        (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "#y_test = np.reshape(y_test, \n",
    "        # (y_test.shape[0],  1))\n",
    "y_pred = model.predict(X_test_input)\n",
    "\n",
    "pred = np.concatenate((X_test, y_pred), axis=1)\n",
    "\n",
    "# Reskaliere die Daten\n",
    "test_rs = pd.DataFrame(scaler.inverse_transform(test_s), columns = test_s.columns)\n",
    "pred_rs = pd.DataFrame(scaler.inverse_transform(pred), columns = test_s.columns)\n",
    "\n",
    "y_test = test_rs['Verbrauch']\n",
    "y_pred = pred_rs['Verbrauch']\n",
    "\n",
    "# Berechne RMSE der Validierungsdaten\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.round(np.sqrt(mse))\n",
    "print(\"Validierungs-Fehler:\")\n",
    "print(\"\\nMSE:\\n %.2lf\" % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axF8FLI5qxkt"
   },
   "outputs": [],
   "source": [
    "def rel_error(df, col1, col2):\n",
    "    df['Error (%)'] = (df[col1] - df[col2]) / df[col1]  * 100\n",
    "    df['Error (%)'] = df['Error (%)'].abs()\n",
    "    return df['Error (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN0q4AodxpPj"
   },
   "outputs": [],
   "source": [
    "# Hilfsfunktion error_table erzeugt Fehlertabelle\n",
    "def error_table(df1, df2, col1, col2, idx):\n",
    "    cols = [df1, df2]\n",
    "    headers = [col1, col2]\n",
    "    # Erzeuge pred aus y_test und y_pred mit Index idx\n",
    "    pred = pd.concat(cols, axis=1, keys=headers) \n",
    "    pred.set_index(idx, inplace=True)\n",
    "    # Füge Fehler-Spalten hinzu\n",
    "    pred['Error'] = np.abs(pred['y_test'] - pred['y_pred'])\n",
    "    pred['Error(%)'] = rel_error(pred, 'y_test', 'y_pred')\n",
    "    pred = pred.astype(float).round(1)\n",
    "    # Füge RMSE hinzu\n",
    "    mse = mean_squared_error(df1, df2)\n",
    "    rmse = np.round(np.sqrt(mse))\n",
    "    pred.index.name = \"RMSE: \" + str(rmse)\n",
    "    return pred\n",
    "\n",
    "# Erzeuge Fehlertabelle für Validierung und gebe sie aus\n",
    "idx = test.index;\n",
    "pred = error_table(pd.DataFrame(y_test), pd.DataFrame(y_pred), 'y_test', 'y_pred', idx)\n",
    "\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf8vmdiCmWPg"
   },
   "outputs": [],
   "source": [
    "pred['Error (%)'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBvPJvxdmZ9N"
   },
   "outputs": [],
   "source": [
    "pred['Error (%)'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTPeFBiX-bga"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_ruAL4TjMF-m",
    "KrKru1jmRN3G"
   ],
   "name": "HandsOnPython_06_NeuronalesNetz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
